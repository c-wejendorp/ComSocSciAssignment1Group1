{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9cf42261",
   "metadata": {},
   "source": [
    "# Assignment 1 group 1\n",
    "Github repo is found at: https://github.com/c-wejendorp/ComSocSciAssignment1Group1\n",
    "<br>\n",
    "Contributions: s204090 45% , s204145 10% and s216135 45%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3751b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries used in this notebook\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "\n",
    "doRequests=False\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79a81f16",
   "metadata": {},
   "source": [
    "#### Part 1: Using web-scraping to gather data\n",
    "We are interested in identifying the researchers that have joined the most important scientific conference in Computational Social Science in 2019. To do this we webscrape the programmes from 2019. They can be found at the following links:<br>\n",
    "- https://2019.ic2s2.org/oral-presentations/\n",
    "- https://2019.ic2s2.org/posters/\n",
    "\n",
    "We scraped the websites using the code below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa9ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "oralPresenters = set()\n",
    "#first link\n",
    "link1=\"https://2019.ic2s2.org/oral-presentations/\"\n",
    "#get the HTML content\n",
    "r = requests.get(link1)\n",
    "#use BeautifulSoup to parse the content. \n",
    "soup = BeautifulSoup(r.content)\n",
    "#locate the HTML tag containing the author names by inspecting the website using at browser.\n",
    "# We see that the names are located in the tag <p> \n",
    "paragraphs=soup.find_all(\"p\")\n",
    "#relevant paragraphs choosen by quick manual inspection\n",
    "paragraphs=paragraphs[3:-7]\n",
    "#we note that each rooom has a \"chairperson\" which WILL NOT be included. \n",
    "\n",
    "for paragraph in paragraphs:\n",
    "    #print(paragraph)\n",
    "    text=str(paragraph)\n",
    "    #spilt by the \"<br>\"\n",
    "    text=re.split(\"<br>|<br/>\",text)    \n",
    "    #locate the names by splitting by \"-\" and choosing idx 2    \n",
    "    for line in text[2:]:        \n",
    "        lineList=re.split(r'\\s*\\–\\s*|\\.s*',line)        \n",
    "        #get each individual name \n",
    "        # we check if lineList has a lenght over 1 as sometimes there is no \".\" btw the authornames and the topic.\n",
    "        # Instead there is a \"–\". Thus we do not include the authors where this is the case. \n",
    "        if len(lineList)>1:\n",
    "            \n",
    "            names=re.split(r',\\s*',lineList[2])    \n",
    "            for name in names:\n",
    "                oralPresenters.add(name)\n",
    "\n",
    "print(f\"We found {len(oralPresenters)} authors when scraping the oral presentations\")     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7d6143",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterPresenters = set()\n",
    "#second link\n",
    "link2= \"https://2019.ic2s2.org/posters/\"\n",
    "#get the HTML content\n",
    "r = requests.get(link2)\n",
    "#use BeautifulSoup to parse the content. \n",
    "soup = BeautifulSoup(r.content)\n",
    "#we locate the HTML tag <ul> in the soup.\n",
    "unOrderedLists=soup.find_all(\"ul\")\n",
    "#locate the HTML tag <li> as the author names are located within these tags. \n",
    "lists=soup.find_all(\"li\")\n",
    "#We did some manual inspection and choose the relevant <li> tags\n",
    "relevantLists=lists[32:-7]\n",
    "\n",
    "for num,item in enumerate(relevantLists):    \n",
    "    text=str(item)   \n",
    "    # find the text between the first \"<li>\" and first\"<span>\"\n",
    "    #text=re.split(r\"\\s*<li>|<span>\",text,maxsplit=2)\n",
    "    text=re.split(r\"\\s*<li>|\\s*</li>|<span>\",text)\n",
    "    # for some reason the first element after the split is \"\" an empty string, so the author names is located at index 1.    \n",
    "    namesString=text[1]\n",
    "    #split the names string into individual names. They are seperated by \",\" and \"and\". However we must bed carefull not to split name such as as \"Alessandro Cossard\" into \"Aless\" and \"ro\"\n",
    "    # thereofore we require the char before \"and\" to be a space. This achieved by: (?<=\\s)    \n",
    "    names=re.split(r\"\\s*,\\s*|(?<=\\s)and\\s*\",namesString) \n",
    "    \n",
    "    for name in names:\n",
    "        #when we inspected the posterPresenters set, it became clear that some unwanted \"names\" were added to the set. An example is: \"<strong>Evolution of Employment in the United States: A Longitudinal Study of Job Polarisation</strong>\"\n",
    "        #thus we decided to only add a name, if the string doesn't contain any typical html notation such as \"<\",\">\" and \"/\"\n",
    "        pattern = re.compile(r'[<>/]')\n",
    "        if pattern.findall(name):\n",
    "            #uncomment below if you want to see the \"names\" that contains unwanted characters             \n",
    "            #print(name)\n",
    "            pass \n",
    "        else: \n",
    "            posterPresenters.add(name)            \n",
    "\n",
    "print(f\"We found {len(posterPresenters)} authors when scraping the poster presentations\")          \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e554bbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique authors:\n",
    "uniqueAuthors=oralPresenters|posterPresenters\n",
    "print(f\"We found {len(uniqueAuthors)} unique researchers in 2019\") \n",
    "\n",
    "#save the names into a csv.\n",
    "f = open('data/initialNames.csv',\"w\")\n",
    "writer = csv.writer(f,lineterminator = '\\n')\n",
    "for name in list(uniqueAuthors):        \n",
    "    writer.writerow([name])\n",
    "f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f488258",
   "metadata": {},
   "source": [
    "During the websscraping we were very careful when extracting names from lines of text.\n",
    "As an example we could not just split a string as \"Jacob SomethingCool, Alessandro Cossard and Lars AnotherName\"\n",
    "carelessly by the pattern \",\" or \"and\" as this would lead to the names:\n",
    "[\"Jacob SomethingCool, Aless, andro Cossard, Lars AnotherName\"] and thus ending up overestimating the number of authors. In the case of the 2019 programmes it was fairly easy to perform some manuel checks, to estimate whether the found number of researchers seemed plausable. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88033942",
   "metadata": {},
   "source": [
    "#### Part 2: Getting data from the Semantic Scholar API\n",
    "In this notebook we will only use the researchers from the conference in 2019. From these 915 conference attendees we end up with 54589 unique IDs when all their collaborators have been found. This is done in the notebook getAuthorIDs.ipynb. \n",
    "\n",
    "When working with the Semantic Scholar API it is only possible to do 100 request pr 5 min. However they do include the option to do a batchsearch of up to 100 IDs in a single request. We did utilze this but the API is very prone to errors, so we potentially miss out on information on a lot of authors. To accomdate for this we create a list to store these \"bad\" responses. For each batch request we create a small dataframe with information about 100 authors and save this into a pkl. This is to limit the size of the files. We then merge all the smaller dataframes into one big one where we extract information needed to create the three smaller dataframes: \"Author dataset\", \"Paper dataset\" and \"Paper abstract dataset\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154bebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets load the IDs of all authors from the csv. \n",
    "allAuthorIDs={}\n",
    "with open('data/allAuthorIDs.csv', 'r') as f:\n",
    "    for line in csv.reader(f):\n",
    "        id = line[0]\n",
    "        name = line[1]\n",
    "        allAuthorIDs[id]=name \n",
    "print(f\"The number of total IDs are {len(allAuthorIDs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa71dd27",
   "metadata": {},
   "source": [
    "The cell below DOES NOT NEED TO BE RAN, it only serves as documentation for how we worked with the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ee54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if doRequests==True:\n",
    "    #we split into appropiate batch sizes and requestsizes. \n",
    "    # we can request 100 times pr 5 min and each request can be on 100 names\n",
    "    numRequests = 100\n",
    "    batchSize = 100\n",
    "    sleepTIme= 60*5\n",
    "\n",
    "    authorList=list(allAuthorIDs.keys())\n",
    "\n",
    "\n",
    "    authorBatches = [authorList[i:i + batchSize] for i in range(0, len(authorList), batchSize)]\n",
    "    requestList = [authorBatches[i:i + numRequests] for i in range(0, len(authorBatches),numRequests)]\n",
    "\n",
    "    BASE_URL = \"https://api.semanticscholar.org/graph/\"\n",
    "    VERSION = \"v1/\"\n",
    "    RESOURCE = \"author/batch\"\n",
    "    my_url = BASE_URL + VERSION + RESOURCE\n",
    "    params = {\n",
    "                \"fields\":\"authorId,name,aliases,citationCount,papers,papers.externalIds,papers.title,papers.abstract,papers.year,papers.citationCount,papers.s2FieldsOfStudy,papers.authors\"}\n",
    "\n",
    "    #This is a pandas df that contains all the relevent info, and it will later be split up into the three required. \n",
    "    #The pandasDF\n",
    "    authorDfColumns=[\"authorId\",\"name\",\"aliases\",\"citationCount\",\"papersId\",\"papersExternalId\",\"papersTitle\",\"papersAbstract\",\"papersYear\",\"papersCitationCount\",\"papers_s2FieldsOfStudy\",\"papersAuthors\"]\n",
    "    authorDF=pd.DataFrame(columns=authorDfColumns)\n",
    "\n",
    "    # sometimes we get an internal server error, so we would like to save these batches for inspection or something else\n",
    "    theBadBunch = []\n",
    "\n",
    "    #pandas is not so fond of this way of appending to a df, so we suppress the warning. \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(action='ignore', category=FutureWarning)    \n",
    "        \n",
    "        for num,request in enumerate(requestList):\n",
    "            print(f\"I am a request number {num+1} out of {len(requestList)}\")\n",
    "            for batchnum,batch in enumerate(request):        \n",
    "                json_data={\"ids\": batch}\n",
    "                r=requests.post(my_url,json=json_data,params = params)\n",
    "\n",
    "                statuscode = r.reason\n",
    "                #print(statuscode)\n",
    "                if statuscode == \"OK\": \n",
    "                    batchauthorDF = authorDF.copy()        \n",
    "                    #try:                  \n",
    "                    for person in r.json():\n",
    "                        if person is not None:\n",
    "                        #check if we recieved usefull information\n",
    "                            if person.get(\"authorId\") is not None:                \n",
    "\n",
    "                                authorDetails = {\n",
    "                                \"authorId\": person[\"authorId\"],      \n",
    "                                \"name\":  person[\"name\"],\n",
    "                                \"aliases\": person[\"aliases\"],\n",
    "                                \"citationCount\": person[\"citationCount\"]}\n",
    "                                #check if the author have written some papers\n",
    "                                if person.get(\"papers\") is not None:\n",
    "                                    papers = person[\"papers\"]\n",
    "                                    #update the authorDetails dict by the update operator: \"|=\"            \n",
    "                                    authorDetails |= {                \n",
    "                                    \"papersId\": [paper['paperId'] for paper in papers],            \n",
    "                                    \"papersExternalId\": [paper['externalIds'] for paper in papers],                       \n",
    "                                    \"papersTitle\": [paper['title'] for paper in papers],  \n",
    "                                    \"papersAbstract\": [paper['abstract'] for paper in papers], \n",
    "                                    \"papersYear\": [paper['year'] for paper in papers], \n",
    "                                    \"papersCitationCount\": [paper['citationCount'] for paper in papers], \n",
    "                                    \"papers_s2FieldsOfStudy\": [paper['s2FieldsOfStudy'] for paper in papers], \n",
    "                                    \"papersAuthor\": [paper['authors'] for paper in papers], \n",
    "                                                                                            }\n",
    "                                #lets add relevant information to our df\n",
    "                                batchauthorDF = batchauthorDF.append(authorDetails, ignore_index=True)\n",
    "                                pickleName = f\"data/syltedeAgurker/agurk{num},{batchnum}.pkl\"\n",
    "                                batchauthorDF.to_pickle(pickleName)\n",
    "                                    \n",
    "                            else:\n",
    "                                print(\"Bad response from API, unable to use. THis is the json:\")\n",
    "                                print(f\"This should be the person {person}\")\n",
    "                                print(\"And this is the json\")\n",
    "                                print(r.json())    \n",
    "                    #except:\n",
    "                        #print(\"I had an error..\")\n",
    "                        \n",
    "\n",
    "                else: \n",
    "                    theBadBunch.append((statuscode,batch))\n",
    "            print(\"I am going to sleep\")        \n",
    "            time.sleep(sleepTIme)\n",
    "            print(\"I woke up\")     \n",
    "\n",
    "    #we save all the list with the author IDs, which did return a bad response from the API; such as 'Internal Server Error'. \n",
    "    with open(\"data/theBadBunch.pkl\", 'wb') as file:\n",
    "        pickle.dump(theBadBunch, file)\n",
    "    print(f'We have failed to retrieve data for {len(theBadBunch)*100} authors, due to errors when requesting')\n",
    "    print(\"These author IDs are now stored in theBadBunch.pkl if want to do something about this\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c36f996f",
   "metadata": {},
   "source": [
    "In the github repo we have provided the pickles needed to create the dataframe below. Thus one can continue executing cells from here. In the cell below we create one big DF that we will use to create the three smaller. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9ecf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just iterate over all syltede agurker ^^\n",
    "directory = 'data/syltedeAgurker' \n",
    "# iterate over files in\n",
    "# that in the directory\n",
    "pandaListen = []\n",
    "for agurk in os.listdir(directory):\n",
    "    agurkLokation = os.path.join(directory, agurk)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(agurkLokation):\n",
    "        unpickled_df=pd.read_pickle(agurkLokation) \n",
    "        pandaListen.append(unpickled_df)\n",
    "\n",
    "df_big = pd.concat(pandaListen, ignore_index=True) \n",
    "print(f\"After we have completed all the batchrequest, we were left with {len(df_big)} out of {len(allAuthorIDs)} IDs\")\n",
    "print(f\"This is not optimal, but one could potentially try doing single ID request as we have stored the IDs in the file 'theBadBunch'.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create the three dataframes requested:\n",
    "# first the authorDataSet dataframe\n",
    "\n",
    "authorDataSet=df_big[[\"authorId\",\"name\",\"aliases\",\"citationCount\",\"papers_s2FieldsOfStudy\"]].copy()\n",
    "\n",
    "authorDataSet[\"fields\"]=[[dictionary[\"category\"] for sublist in author for dictionary in sublist] for author in authorDataSet[\"papers_s2FieldsOfStudy\"]]\n",
    "\n",
    "authorDataSet[\"mainField\"]=authorDataSet[\"fields\"].apply(lambda lst: max(set(lst), key=lst.count,default=\"None\"))\n",
    "\n",
    "authorDataSetFinal=authorDataSet[[\"authorId\",\"name\",\"aliases\",\"citationCount\",\"mainField\"]].copy()\n",
    "\n",
    "authorDataSetFinal.rename(columns = {'mainField':'field'}, inplace = True)\n",
    "authorDataSetFinal.to_pickle(\"data/authorDataSet.pkl\")\n",
    "print(f\"The lenght of the final Author dataframe is: {len(authorDataSetFinal)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c679ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the authorDataSetFinal df from memory.\n",
    "del [[authorDataSetFinal]]\n",
    "gc.collect()\n",
    "authorDataSetFinal = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af5e684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the two paper dataframes.\n",
    "#We start by creating a big paper df. \n",
    "columnNames=[\"papersId\",\"papersTitle\",\"papersYear\",\"papersExternalId\",\"papersCitationCount\",\"papers_s2FieldsOfStudy\",\"papersAuthor\",\"papersAbstract\"]\n",
    "\n",
    "bigPaperDataSet=df_big[columnNames].copy()\n",
    "#The dataframe is now such that reach row correspond to a single author. This means that an entry in \"papersId\" is actually a list with the different paper IDs.\n",
    "# Thus we create a new df by zipping the lists in each row. \n",
    "# We then add each paper to a paperDF by iterating over the zip.  \n",
    "# this is done in the next cell.\n",
    "\n",
    "#to free some memory we delete the df_big, as we dont need it anymore. \n",
    "del [[df_big]]\n",
    "gc.collect()\n",
    "df_big = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bb8175",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionaryList=[]\n",
    " \n",
    "for index, row in bigPaperDataSet.iterrows():\n",
    "    rowinformation=[row[column] for column in columnNames]\n",
    "    zipped=zip(*rowinformation)    \n",
    "    for paper in zipped:\n",
    "        paperdictionary = dict(zip(columnNames, paper))\n",
    "        dictionaryList.append(paperdictionary)\n",
    "\n",
    "bigPaperDataSet = pd.DataFrame.from_dict(dictionaryList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbb1eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we extract information from the bigPaperDataSet to creat the two last dataframes:\n",
    "\n",
    "PaperDataSet=bigPaperDataSet[[\"papersId\",\"papersTitle\",\"papersYear\",\"papersExternalId\",\"papersCitationCount\",\"papers_s2FieldsOfStudy\",\"papersAuthor\"]]\n",
    "PaperDataSet.to_pickle(\"data/paperDataSet.pkl\")\n",
    "\n",
    "PaperAbstractDataSet=bigPaperDataSet[[\"papersId\",\"papersAbstract\"]]\n",
    "PaperAbstractDataSet.to_pickle(\"data/paperAbstractDataSet.pkl\")\n",
    "\n",
    "print(f\"The lenght of the final paper dataframe is: {len(PaperDataSet)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b15a1608",
   "metadata": {},
   "source": [
    "Part 2 summary.\n",
    "\n",
    "We started by using the 54589 unique IDs which came from the 2019 conference speakers and their collaborators.\n",
    "This number was then reduced to 22375 due to a failure prone API. The ones we could not retrieve data on is saved such that they can be requested if needed at a later time. \n",
    "\n",
    "The final lenght of the author dataframe is: 22375\n",
    "The final lenght of the paper dataframe is: 1125521"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae72de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all dataframes from memory\n",
    "del [[bigPaperDataSet,PaperAbstractDataSet]]\n",
    "gc.collect()\n",
    "bigPaperDataSet = pd.DataFrame()\n",
    "PaperAbstractDataSet = pd.DataFrame()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d287f4d",
   "metadata": {},
   "source": [
    "# Exercise 3: Law of large numbers\n",
    "## Normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7a045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "np.random.seed(2)\n",
    "random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37caa93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10_000\n",
    "\n",
    "X = np.arange(0, num_samples) # ranges used for plotting\n",
    "X_ = np.arange(1, num_samples+1, 1_000)\n",
    "\n",
    "def analyze_sample(Y):  \n",
    "    \"\"\"Returns all relevant stats for a sample Y.\"\"\"\n",
    "    n = len(Y)\n",
    "  \n",
    "    # 3-4) Cumulative average and standard error\n",
    "    cum_avg = [np.mean(Y[:i]) for i in range(1, n+1)]\n",
    "    cum_SE = [np.std(Y[:i]) / np.sqrt(i) for i in range(1, n+1)]\n",
    "\n",
    "    # 5) mean and median\n",
    "    mean, median = np.mean(Y), np.median(Y)\n",
    "    \n",
    "    # 7) Cumulative median\n",
    "    cum_med = [np.median(Y[:i]) for i in range(1, n+1)]\n",
    "\n",
    "    return cum_avg, cum_SE, cum_med, mean, median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400a290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Gaussian sampling\n",
    "mu, sigma = 0, 4\n",
    "Y = np.random.standard_normal(num_samples) * sigma + mu\n",
    "cum_avg, cum_SE, cum_med, mean, median = analyze_sample(Y)\n",
    "\n",
    "# 2) Plot distribution\n",
    "plt.hist(Y, bins=100, density=True)\n",
    "plt.title('Gaussian distribution')\n",
    "plt.show()\n",
    "\n",
    "# 6) Plot cumulative average and standard error\n",
    "plt.title('Cumulative Average of Gaussian Distribution')\n",
    "plt.xlabel('Sample Size'); plt.ylabel('Average')\n",
    "# X = np.arange(0, num_samples)\n",
    "plt.plot(X, cum_avg, label='Cumulative average')\n",
    "\n",
    "# X_ = np.arange(1, num_samples+1, 1_000)  # Error bar - Only plot every 1000th point for ease of viewing\n",
    "cum_avg = [cum_avg[x] for x in X_]\n",
    "cum_SE = [cum_SE[x] for x in X_]\n",
    "plt.errorbar(X_, cum_avg, yerr=cum_SE, fmt='o', capsize=3, label='Standard error')\n",
    "plt.axhline(mean, color='r', label='Mean')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 8) Plot cumulative median and distribution median\n",
    "plt.title('Cumulative Median of Gaussian Distribution')\n",
    "plt.xlabel('Sample Size'); plt.ylabel('Median')\n",
    "plt.plot(X, cum_med, label='Cumulative median')\n",
    "plt.axhline(median, color='r', label='Median')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6cbb4ee",
   "metadata": {},
   "source": [
    "## Pareto distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d75518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Pareto sampling\n",
    "x_m = 1 # scale x_m/m parameter\n",
    "alpha = 0.5 # shape alpha/a parameter\n",
    "Y = (np.random.pareto(alpha, num_samples) + 1) * x_m\n",
    "cum_avg, cum_SE, cum_med, mean, median = analyze_sample(Y)\n",
    "\n",
    "# 2) Plot distribution\n",
    "plt.hist(Y, bins=np.arange(0, 30, 1), density=True)\n",
    "plt.title('Pareto distribution')\n",
    "plt.show()\n",
    "\n",
    "# 6) Plot cumulative average\n",
    "plt.title('Cumulative Average of Pareto Distribution')\n",
    "plt.xlabel('Sample Size'); plt.ylabel('Average')\n",
    "plt.plot(X, cum_avg, label='Cumulative average')  # LEARNING MOMENT max(cum_avg) return 300000, which afterwards drops towards zero\n",
    "\n",
    "cum_avg = [cum_avg[x] for x in X_]  # Error bars\n",
    "cum_SE = [cum_SE[x] for x in X_]\n",
    "plt.errorbar(X_, cum_avg, yerr=cum_SE, fmt='o', capsize=3, label='Standard error')\n",
    "plt.axhline(mean, color='r', label='Mean')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 7) Plot cumulative median\n",
    "plt.title('Cumulative Median of Pareto Distribution')\n",
    "plt.xlabel('Sample Size'); plt.ylabel('Median')\n",
    "plt.plot(X, cum_med, label='Cumulative median')\n",
    "plt.axhline(median, color='r', label='Median')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad541e91",
   "metadata": {},
   "source": [
    "## Lognormal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63824a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Pareto sampling by inverse transform sampling of uniform distribution\n",
    "mu = 0 \n",
    "sigma = 4 \n",
    "Y = np.exp(np.random.standard_normal(num_samples) * sigma + mu)\n",
    "cum_avg, cum_SE, cum_med, mean, median = analyze_sample(Y)\n",
    "\n",
    "# 2) Plot distribution\n",
    "plt.hist(Y, bins=np.arange(0, 30, 1), density=True)\n",
    "plt.title('Lognormal distribution')\n",
    "plt.show()\n",
    "\n",
    "# 6) Plot cumulative average\n",
    "plt.title('Cumulative Average of Lognormal Distribution')\n",
    "plt.xlabel('Sample Size'); plt.ylabel('Average')\n",
    "plt.plot(X, cum_avg, label='Cumulative average')\n",
    "\n",
    "cum_avg = [cum_avg[x] for x in X_]  # Error bars\n",
    "cum_SE = [cum_SE[x] for x in X_]\n",
    "plt.errorbar(X_, cum_avg, yerr=cum_SE, fmt='o', capsize=3, label='Standard error')\n",
    "plt.axhline(mean, color='r', label='Mean')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 7) Plot cumulative median\n",
    "plt.title('Cumulative Median of Lognormal Distribution')\n",
    "plt.xlabel('Sample Size'); plt.ylabel('Median')\n",
    "plt.plot(X, cum_med, label='Cumulative median')\n",
    "plt.axhline(median, color='r', label='Median')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "157b8457",
   "metadata": {},
   "source": [
    "## Citations from 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caa0ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "citation_count = list(PaperDataSet['papersCitationCount'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37269e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = random.sample(citation_count, num_samples)\n",
    "cum_avg, cum_SE, cum_med, mean, median = analyze_sample(Y)\n",
    "\n",
    "# 2) Plot distribution\n",
    "plt.hist(Y, bins=np.arange(0, 30, 1), density=True)\n",
    "plt.title('Citation distribution')\n",
    "plt.show()\n",
    "\n",
    "# 6) Plot cumulative average\n",
    "plt.title('Cumulative Average of Citation Distribution')\n",
    "plt.xlabel('Sample Size'); plt.ylabel('Average')\n",
    "plt.plot(X, cum_avg, label='Cumulative average')\n",
    "\n",
    "cum_avg = [cum_avg[x] for x in X_]  # Error bars\n",
    "cum_SE = [cum_SE[x] for x in X_]\n",
    "plt.errorbar(X_, cum_avg, yerr=cum_SE, fmt='o', capsize=3, label='Standard error')\n",
    "plt.axhline(mean, color='r', label='Mean')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 7) Plot cumulative median\n",
    "plt.title('Cumulative Median of Citation Distribution')\n",
    "plt.xlabel('Sample Size'); plt.ylabel('Median')\n",
    "plt.plot(X, cum_med, label='Cumulative median')\n",
    "plt.axhline(median, color='r', label='Median')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e090acc",
   "metadata": {},
   "source": [
    "Compare the evolution of the cumulative average for the Gaussian, Pareto and LogNormal distribution. What do you observe? Would you expect these results? Why?\n",
    "- Gaussian is not heavy tailed, so follows the Law of Large Numbers. Pareto has crazy outliers, due to the 80/20 rule of which it was defined from. It is defined to be heavy tailed. Thus a single outlier can have a huge impact on the cumulative mean. The same applies to Lognormal, as plots behave similar. This could be due to the exponential increasing the impact of outliers.\n",
    "\n",
    "Compare the cumulative median vs the cumulative average for the three distributions. What do you observe? Can you draw any conclusions regarding which statistics (the mean or the median) is more usfeul in the different cases?\n",
    "- In general median is more robust to outliers. Thus, median is most usefull in heavy tailed distributions. \n",
    "\n",
    "Consider the plots you made using the citation count data in point 14. What do you observe? What are the implications?\n",
    "- The distribution is heavy tailed, and thus the mean is not a good representation of the data. The median is more usefull.\n",
    "\n",
    "What do you think are the main take-home message of this exercise?\n",
    "- Papers citations are apparently heavy tailed. Heavy tailed distributions are not well represented by the mean, and thus median is more usefull. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comSocSci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "a081079ee5b07d416a411aa15740ce1fe1934fc3e84841114a520aad35961001"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
