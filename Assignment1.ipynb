{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 group 1\n",
    "Github repo is found at: https://github.com/c-wejendorp/ComSocSciAssignment1Group1\n",
    "<br>\n",
    "Contributions: s204090 0% , s204145 0% and s216135 0%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries used in this notebook\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Using web-scraping to gather data\n",
    "We are interested in identifying the researchers that have joined the most important scientific conference in Computational Social Science in 2019. To do this we webscrape the programmes from 2019. They can be found at the following links:<br>\n",
    "- https://2019.ic2s2.org/oral-presentations/\n",
    "- https://2019.ic2s2.org/posters/\n",
    "\n",
    "We scraped the websites using the code below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oralPresenters = set()\n",
    "#first link\n",
    "link1=\"https://2019.ic2s2.org/oral-presentations/\"\n",
    "#get the HTML content\n",
    "r = requests.get(link1)\n",
    "#use BeautifulSoup to parse the content. \n",
    "soup = BeautifulSoup(r.content)\n",
    "#locate the HTML tag containing the author names by inspecting the website using at browser.\n",
    "# We see that the names are located in the tag <p> \n",
    "paragraphs=soup.find_all(\"p\")\n",
    "#relevant paragraphs choosen by quick manual inspection\n",
    "paragraphs=paragraphs[3:-7]\n",
    "#we note that each rooom has a \"chairperson\" which WILL NOT be included. \n",
    "\n",
    "for paragraph in paragraphs:\n",
    "    #print(paragraph)\n",
    "    text=str(paragraph)\n",
    "    #spilt by the \"<br>\"\n",
    "    text=re.split(\"<br>|<br/>\",text)\n",
    "    #print(text)\n",
    "    #print(re.split(r'\\s*\\–\\s*|\\.s*',text[0]))\n",
    "    #locate the names by splitting by \"-\" and choosing idx 2    \n",
    "    for line in text[2:]:        \n",
    "        lineList=re.split(r'\\s*\\–\\s*|\\.s*',line)        \n",
    "        #get each individual name \n",
    "        # we check if lineList has a lenght over 1 as sometimes there is no \".\" btw the authornames and the topic.\n",
    "        # Instead there is a \"–\". Thus we do not include the authors where this is the case. \n",
    "        if len(lineList)>1:\n",
    "            \n",
    "            names=re.split(r',\\s*',lineList[2])    \n",
    "            for name in names:\n",
    "                oralPresenters.add(name)\n",
    "\n",
    "print(f\"We found {len(oralPresenters)} authors when scraping the oral presentations\")     \n",
    "print(f\"we had one odd entry in our set: '(Moved to 3D Text Analysis) Ivan Smirnov', which was easily fixed\")\n",
    "oralPresenters.add('Ivan Smirnov')\n",
    "oralPresenters.remove('(Moved to 3D Text Analysis) Ivan Smirnov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterPresenters = set()\n",
    "#second link\n",
    "link2= \"https://2019.ic2s2.org/posters/\"\n",
    "#get the HTML content\n",
    "r = requests.get(link2)\n",
    "#use BeautifulSoup to parse the content. \n",
    "soup = BeautifulSoup(r.content)\n",
    "#we locate the HTML tag <ul> in the soup.\n",
    "unOrderedLists=soup.find_all(\"ul\")\n",
    "#locate the HTML tag <li> as the author names are located within these tags. \n",
    "lists=soup.find_all(\"li\")\n",
    "#We did some manual inspection and choose the relevant <li> tags\n",
    "relevantLists=lists[32:-7]\n",
    "\n",
    "for num,item in enumerate(relevantLists):    \n",
    "    text=str(item)   \n",
    "    # find the text between the first \"<li>\" and first\"<span>\"\n",
    "    #text=re.split(r\"\\s*<li>|<span>\",text,maxsplit=2)\n",
    "    text=re.split(r\"\\s*<li>|\\s*</li>|<span>\",text)\n",
    "    # for some reason the first element after the split is \"\" an empty string, so the author names is located at index 1.    \n",
    "    namesString=text[1]\n",
    "    #split the names string into individual names. They are seperated by \",\" and \"and\". However we must bed carefull not to split name such as as \"Alessandro Cossard\" into \"Aless\" and \"ro\"\n",
    "    # thereofore we require the char before \"and\" to be a space. This achieved by: (?<=\\s)    \n",
    "    names=re.split(r\"\\s*,\\s*|(?<=\\s)and\\s*\",namesString) \n",
    "    \n",
    "    for name in names:\n",
    "        #when we inspected the posterPresenters set, it became clear that some unwanted \"names\" were added to the set. An example is: \"<strong>Evolution of Employment in the United States: A Longitudinal Study of Job Polarisation</strong>\"\n",
    "        #thus we decided to only add a name, if the string doesn't contain any typical html notation such as \"<\",\">\" and \"/\"\n",
    "        pattern = re.compile(r'[<>/]')\n",
    "        if pattern.findall(name):\n",
    "            #uncomment below if you want to see the \"names\" that contains unwanted characters             \n",
    "            #print(name)\n",
    "            pass \n",
    "        else: \n",
    "            posterPresenters.add(name)            \n",
    "\n",
    "print(f\"We found {len(posterPresenters)} authors when scraping the poster presentations\")          \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique authors:\n",
    "uniqueAuthors=oralPresenters|posterPresenters\n",
    "print(f\"We found {len(uniqueAuthors)} unique researchers in 2019\") \n",
    "\n",
    "#save the names into a csv.\n",
    "f = open('data/initialNames.csv',\"w\")\n",
    "writer = csv.writer(f,lineterminator = '\\n')\n",
    "for name in list(uniqueAuthors):    \n",
    "    writer.writerow([name])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the websscraping we were very careful when extracting names from lines of text.\n",
    "As an example we could not just split a string as \"Jacob SomethingCool, Alessandro Cossard and Lars AnotherName\"\n",
    "carelessly by the pattern \",\" or \"and\" as this would lead to the names:\n",
    "[\"Jacob SomethingCool, Aless, andro Cossard, Lars AnotherName\"] and thus ending up overestimating the number of authors. In the case of the 2019 programmes it was fairly easy to perform some manuel checks, to estimate whether the found number of researchers seemed plausable. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Getting data from the Semantic Scholar API\n",
    "In this notebook we will only use the researchers from the conference in 2019, as this notebook othwise will take hours to run. From these 914 conference attendees we end up with 54589 unique IDs when all their collaborators have been found.\n",
    "  \n",
    " When searching for a given author's name the API will return multiple hits. We have choosen to only use the first returned name and ID. We also note that when using the API to search for author names, only one name can be send pr request as to opposed to when searching for authors based on IDs, where one can include up 100 IDs in a single request. As the request sometimes fails, we also keep track of the \"faulty names\". If interested look into the the getAuthorIDs.ipynb\n",
    "\n",
    "For now we will focus on the task of creating the three dataframes; \"Author dataset\", \"Paper dataset\" and \"Paper abstract dataset\". As we are going to be doing quite a lot of requests, we continuously saves smaller data frames as pickles in the folder \"syltedeAgurker\". We later then merge these dataframes and create the before mentioned dataframes. The starting point will b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets load the IDs of all authors from the csv. \n",
    "allAuthorIDs={}\n",
    "with open('data/allAuthorIDs.csv', 'r') as f:\n",
    "    for line in csv.reader(f):\n",
    "        id = line[0]\n",
    "        name = line[1]\n",
    "        allAuthorIDs[id]=name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we split into appropiate batch sizes and requestsizes. \n",
    "# we can request 100 times pr 5 min and each request can be on 100 names\n",
    "numRequests = 100\n",
    "batchSize = 100\n",
    "sleepTIme= 60*5\n",
    "\n",
    "authorList=list(allAuthorIDs.keys())\n",
    "\n",
    "\n",
    "authorBatches = [authorList[i:i + batchSize] for i in range(0, len(authorList), batchSize)]\n",
    "requestList = [authorBatches[i:i + numRequests] for i in range(0, len(authorBatches),numRequests)]\n",
    "\n",
    "BASE_URL = \"https://api.semanticscholar.org/graph/\"\n",
    "VERSION = \"v1/\"\n",
    "RESOURCE = \"author/batch\"\n",
    "my_url = BASE_URL + VERSION + RESOURCE\n",
    "params = {\n",
    "            \"fields\":\"authorId,name,aliases,citationCount,papers,papers.externalIds,papers.title,papers.abstract,papers.year,papers.citationCount,papers.s2FieldsOfStudy,papers.authors\"}\n",
    "\n",
    "#This is a pandas df that contains all the relevent info, and it will later be split up into the three required. \n",
    "#The pandasDF\n",
    "authorDfColumns=[\"authorId\",\"name\",\"aliases\",\"citationCount\",\"papersId\",\"papersExternalId\",\"papersTitle\",\"papersAbstract\",\"papersYear\",\"papersCitationCount\",\"papers_s2FieldsOfStudy\",\"papersAuthors\"]\n",
    "authorDF=pd.DataFrame(columns=authorDfColumns)\n",
    "\n",
    "# sometimes we get an internal server error, so we would like to save these batches for inspection or something else\n",
    "theBadBunch = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a request number 4 out of 6\n",
      "I am going to sleep\n",
      "I woke up\n",
      "I am a request number 5 out of 6\n",
      "I am going to sleep\n",
      "I woke up\n",
      "I am a request number 6 out of 6\n",
      "I am going to sleep\n",
      "I woke up\n"
     ]
    }
   ],
   "source": [
    "#pandas is not so fond of this way of appending to a df, so we suppress the warning. \n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)    \n",
    "    \n",
    "    for num,request in enumerate(requestList):\n",
    "        print(f\"I am a request number {num+1} out of {len(requestList)}\")\n",
    "        for batchnum,batch in enumerate(request):        \n",
    "            json_data={\"ids\": batch}\n",
    "            r=requests.post(my_url,json=json_data,params = params)\n",
    "\n",
    "            statuscode = r.reason\n",
    "            #print(statuscode)\n",
    "            if statuscode == \"OK\": \n",
    "                batchauthorDF = authorDF.copy()        \n",
    "                #try:                  \n",
    "                for person in r.json():\n",
    "                    if person is not None:\n",
    "                    #check if we recieved usefull information\n",
    "                        if person.get(\"authorId\") is not None:                \n",
    "\n",
    "                            authorDetails = {\n",
    "                            \"authorId\": person[\"authorId\"],      \n",
    "                            \"name\":  person[\"name\"],\n",
    "                            \"aliases\": person[\"aliases\"],\n",
    "                            \"citationCount\": person[\"citationCount\"]}\n",
    "                            #check if the author have written some papers\n",
    "                            if person.get(\"papers\") is not None:\n",
    "                                papers = person[\"papers\"]\n",
    "                                #update the authorDetails dict by the update operator: \"|=\"            \n",
    "                                authorDetails |= {                \n",
    "                                \"papersId\": [paper['paperId'] for paper in papers],            \n",
    "                                \"papersExternalId\": [paper['externalIds'] for paper in papers],                       \n",
    "                                \"papersTitle\": [paper['title'] for paper in papers],  \n",
    "                                \"papersAbstract\": [paper['abstract'] for paper in papers], \n",
    "                                \"papersYear\": [paper['year'] for paper in papers], \n",
    "                                \"papersCitationCount\": [paper['citationCount'] for paper in papers], \n",
    "                                \"papers_s2FieldsOfStudy\": [paper['s2FieldsOfStudy'] for paper in papers], \n",
    "                                \"papersAuthor\": [paper['authors'] for paper in papers], \n",
    "                                                                                        }\n",
    "                            #lets add relevant information to our df\n",
    "                            batchauthorDF = batchauthorDF.append(authorDetails, ignore_index=True)\n",
    "                            pickleName = f\"data/syltedeAgurker/agurk{num},{batchnum}.pkl\"\n",
    "                            batchauthorDF.to_pickle(pickleName)\n",
    "                                \n",
    "                        else:\n",
    "                            print(\"Bad response from API, unable to use. THis is the json:\")\n",
    "                            print(f\"This should be the person {person}\")\n",
    "                            print(\"And this is the json\")\n",
    "                            print(r.json())    \n",
    "                #except:\n",
    "                    #print(\"I had an error..\")\n",
    "                       \n",
    "\n",
    "            else: \n",
    "                theBadBunch.append((statuscode,batch))\n",
    "        print(\"I am going to sleep\")        \n",
    "        time.sleep(sleepTIme)\n",
    "        print(\"I woke up\")     \n",
    "    #save the DF    \n",
    "    #authorDF.to_pickle(\"data/authorDFnewTrial.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have failed to retrieve data for 33100 authors, due to errors when requesting\n",
      "These author IDs are now stored in theBadBunch.pkl if want to do something about this\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"data/theBadBunch.pkl\", 'wb') as file:\n",
    "    pickle.dump(theBadBunch, file)\n",
    "print(f'We have failed to retrieve data for {len(theBadBunch)*100} authors, due to errors when requesting')\n",
    "print(\"These author IDs are now stored in theBadBunch.pkl if want to do something about this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "331"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(theBadBunch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comSocSci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a081079ee5b07d416a411aa15740ce1fe1934fc3e84841114a520aad35961001"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
