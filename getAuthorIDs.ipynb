{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Find author id by searching the authors name. \n",
    "# Some names yield multiple results corresponding to multiple persons with the same name\n",
    "# We take the first ID connected to a name. \n",
    "\n",
    "# This is the URL to use API. Unfortunatly we can only request one author at a time, when we search by name\n",
    "BASE_URL = \"https://api.semanticscholar.org/graph/\"\n",
    "VERSION = \"v1/\"\n",
    "RESOURCE = \"author/search\"\n",
    "my_url = BASE_URL + VERSION + RESOURCE\n",
    "\n",
    "#We make our set of author names found previously into a list. \n",
    "authors=[]\n",
    "with open(\"data/initialNames.csv\", 'r') as f:\n",
    "    for name in csv.reader(f):\n",
    "        authors.append(name)     \n",
    "\n",
    "\n",
    "# we want to write to a csv file, so we have the data saved.\n",
    "f = open('authorIDs.csv',\"w\")\n",
    "writer = csv.writer(f,lineterminator = '\\n')\n",
    "\n",
    "\n",
    "# dictonaries to save information in. \n",
    "authorIDs={}\n",
    "faultyNames=[]\n",
    "\n",
    "# we can only request 100 times pr 5 min so we split up the list into chuncks of 100=n\n",
    "# and implement timer\n",
    "\n",
    "chunkSize = 100\n",
    "sleepTIme= 60*5\n",
    "authorChunks = [authors[i:i + chunkSize] for i in range(0, len(authors), chunkSize)]\n",
    "#begin performing the requests\n",
    "for chunkNum,chunk in enumerate(authorChunks):\n",
    "    for author in chunk:\n",
    "        params = {'query':author,}\n",
    "        r = requests.get(my_url, params=params)\n",
    "        #check if we get a valid response and f not we keep track of the name causing problems. \n",
    "        if r.reason==\"OK\":\n",
    "            #check if there is data  in the expected place if not we keep track of the name causing problems. \n",
    "            if (id := r.json()[\"data\"]) != []: \n",
    "                #we assume only one person pr name is enough                    \n",
    "                authorIDs[id[0][\"authorId\"]] = id[0][\"name\"]\n",
    "                writer.writerow([id[0][\"authorId\"], id[0][\"name\"]])    \n",
    "            else: \n",
    "                faultyNames.append((author,r.json()))\n",
    "                writer.writerow([author,\"faulty\"])\n",
    "                print(author)\n",
    "        else:\n",
    "            faultyNames.append((author,r.json()))\n",
    "            print(author)\n",
    "            writer.writerow([author,\"faulty\"])\n",
    "            \n",
    "    print(f\"I have requested {chunkNum+1} chunks out of {len(authorChunks)}\")        \n",
    "    time.sleep(sleepTIme)\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets load the IDs of the \"orginal\" authors from the csv. \n",
    "authorIDs={}\n",
    "with open(\"data/authorIDs.csv\", 'r') as f:\n",
    "    for line in csv.reader(f):\n",
    "        if line[1]!=\"faulty\":\n",
    "            id = line[0]\n",
    "            name = line[1]\n",
    "            authorIDs[id]=name  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now for each of the authors we will go through their published papers\n",
    "#and find all their collaborators' IDs\n",
    "# NOTE that their is a limit to 100 authors pr request  \n",
    "\n",
    "collaboratorsIDs={}\n",
    "numRequests = 100\n",
    "batchSize = 100\n",
    "sleepTIme= 60*5\n",
    "authorList=list(authorIDs.keys())\n",
    "authorBathces = [authorList[i:i + batchSize] for i in range(0, len(authorList), batchSize)]\n",
    "requestList = [authorBathces[i:i + numRequests] for i in range(0, len(authorBathces),numRequests)]\n",
    "\n",
    "\n",
    "\n",
    "BASE_URL = \"https://api.semanticscholar.org/graph/\"\n",
    "VERSION = \"v1/\"\n",
    "RESOURCE = \"author/batch\"\n",
    "my_url = BASE_URL + VERSION + RESOURCE\n",
    "params = {\n",
    "            \"fields\":\"papers.authors\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the actuall requests. \n",
    "# we want to write to a csv file, so we have the data saved.\n",
    "f = open('data/collaboratorIDs.csv',\"w\")\n",
    "writer = csv.writer(f,lineterminator = '\\n')\n",
    "\n",
    "orgAuthorLookedUp = 0\n",
    "for request in requestList:\n",
    "    for batch in request:        \n",
    "        json_data={\"ids\": batch}\n",
    "        r=requests.post(my_url,json=json_data,params = params)\n",
    "        # the following is  the structure of the returned list. \n",
    "        # Each element contains a dict where the key \"papers\" contains a list of dicts(one for each paper) and in these dicts the key \"authors\" is a list of dicts of authors of the paper. These dicts have keys \"authorId\" and \"name\". We are after these two values. \n",
    "        for orgAuthor in r.json():\n",
    "            #try: \n",
    "            if orgAuthor[\"papers\"] != []:\n",
    "                orgAuthorLookedUp+=1\n",
    "\n",
    "                for paper in orgAuthor[\"papers\"]:                            \n",
    "                    for collaborator in paper[\"authors\"]:\n",
    "                        id = collaborator[\"authorId\"]\n",
    "                        name =collaborator[\"name\"]                    \n",
    "                        collaboratorsIDs[id]=name\n",
    "                        writer.writerow([id,name])\n",
    "            else:\n",
    "                print(f\"author {orgAuthor} has no papers(?)\")\n",
    "                \n",
    "            #except:\n",
    "            #    print(r.json())\n",
    "            #    print(orgAuthor)\n",
    "            #    pass\n",
    "    #time.sleep(sleepTIme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally we merge the \"orginal\" author dict and the \"collaborator\" author dict into one and save this to csv.\n",
    "allAuthors = authorIDs | collaboratorsIDs\n",
    "f = open('data/allAuthorIDs.csv',\"w\")\n",
    "writer = csv.writer(f,lineterminator = '\\n')\n",
    "for id in list(allAuthors.keys()):\n",
    "    writer.writerow([id,allAuthors[id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Out of {len(authorIDs)} authors, {len(authorIDs)-orgAuthorLookedUp} didn't publish a paper  \")\n",
    "print(f'We found a total of {len(allAuthors)} unique authors in total when including the collaborators')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comSocSci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a081079ee5b07d416a411aa15740ce1fe1934fc3e84841114a520aad35961001"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
